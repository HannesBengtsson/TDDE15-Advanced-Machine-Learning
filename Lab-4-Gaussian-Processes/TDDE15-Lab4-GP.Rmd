---
title: "TDDE15 Lab4 Gaussian Processes"
author: "Hannes Bengtsson"
date: "2023-10-16"
output:
  pdf_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

# Task 1 Implementing GP Regression
## Description 1.1 - Implementing Algorithm 2.1
This first exercise will have you writing your own code for the Gaussian process regression model:
$$
y = f(x) + \epsilon \text{ with } \epsilon \sim N(0,\sigma^{2}_{n}) \text{ and } f \sim GP(0,k(x,x^{'}))
$$
You must implement Algorithm 2.1 on page 19 of Rasmussen and Willams’ book. The algorithm uses the Cholesky decomposition (*chol* in R) to attain numerical stability. Note that L in the algorithm is a lower triangular matrix, whereas the R function returns an upper triangular matrix. So, you need to transpose the output of the R function. In the algorithm, the notation **A/b** means the vector **x** that solves the equation **Ax = b** (see p. xvii in the book). This is implemented in R with the help of the function *solve*.

Write your own code for simulating from the posterior distribution of *f* using the squared exponential kernel. The function (name it *posteriorGP*) should return a vector with the posterior mean and variance of *f*, both evaluated at a set of *x*-values ($X_*$). You can assume that the prior mean of *f* is zero for all *x*. The function should have the following inputs:
\begin{itemize}
  \item X: Vector of training inputs.
  \item y: Vector of training targets/outputs.
  \item XStar: Vector of inputs where the posterior distribution is evaluated, i.e. $X_{*}$.
  \item sigmaNoise: Noise standard deviation $\sigma_{n}$.
  \item k: Covariance function or kernel. That is, the kernel should be a separate function (see the file GaussianProcesses.R on the course web page).
\end{itemize}

## Solution 1.1 - Implementing Algorithm 2.1
Please see the code below.
```{r, echo = FALSE}
# Install and import necessary packages for the lab
library(kernlab)
library(ggplot2)
library(AtmRay)

# Squared exponential kernel function
SEkernel <- function(x1, x2, sigmaf = 1, l = 1) {
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA, n1, n2)
  for (i in 1:n2) {
    K[,i] <- sigmaf^2 * exp(-0.5 * ((x1 - x2[i]) / l)^2)
  }
  return(K)
}

# Function simulating the posterior distribution of f
posteriorGP <- function (x, y, xStar, sigmaNoise, k, ...) {
  # Number of observations
  n <- length(x)
  
  # Calculate the covariance matrices:
  # k(X, X), k(X, X*), k(X*, X*)
  K <- k(x, x, ...)
  KStar <- k(x, xStar, ...)
  KStarStar <- k(xStar, xStar, ...)
  
  # Calculate the lower triangular matrix L
  L <- t(chol(K + (sigmaNoise ^ 2) * diag(n)))
  
  # Calculate alpha
  alpha <- solve(t(L), solve(L, y))
  
  # Calculate the posterior mean for f
  f_mean <- t(KStar) %*% alpha
  
  # Compute the posterior covariance matrix of f
  v <- solve(L, KStar)
  f_cov <- KStarStar - t(v) %*% v
  
  # The diagonal in the covariance matrix gives us the variance
  f_var <- diag(f_cov)
  
  # Compute the Log Marginal Likelihood
  logmar <- -0.5 * (t(y) %*% alpha) - sum(log(diag(L))) - (n/2) * log(2*pi)
  
  # Return a vector with the posterior mean and variance of f
  return(list(Mean=f_mean[,1], Variance=f_var, LogMar=as.numeric(logmar)))
}
```

## Description 1.2 - One Observation Mean and Probability Bands for f
Now, let the prior hyperparameters be $\sigma_{f} = 1$ and $l = 0.3$. Update this prior with a single observation: $(x, y) = (0.4, 0.719)$. Assume that $\sigma_{n} = 0.1$. Plot the posterior mean of *f*
over the interval $x \in [-1, 1]$. Plot also 95 % probability (pointwise) bands for *f*.


## Solution 1.2 - One Observation Mean and Probability Bands for f
```{r, echo = FALSE}
# Simulating some data
obs <- data.frame(x = 0.4, y = 0.719)
sigmaf <- 1
ell <- 0.3
sigmaNoise <- 0.1

x_interval <- seq(-1,1,length=100)

f_posterior <- posteriorGP(x = obs$x,
                           y = obs$y,
                           xStar = x_interval,
                           sigmaNoise = sigmaNoise,
                           k = SEkernel,
                           sigmaf=sigmaf,
                           l = ell)

# A function to plot our posterior mean with 95% probability bands
plot_post_mean <- function(mean, variance, interval, observations) {
  
  # 95% probability bands for f corresponds to 1.96 standard deviations
  prob_bands <-  list (upper = mean + 1.96 * sqrt(variance),
                       lower = mean - 1.96 * sqrt(variance))
  
  # Set ylim so that we include our probability bands
  ylim <- c(-4,4) # Alt: min(prob_bands$lower) and max(prob_bands$upper)
  
  # Plot the mean
  plot(x = interval,
       y = mean,
       type = 'l',
       col = 'black',
       ylab = 'Y',
       xlab = "X",
       ylim = ylim)
  
  # Draw the probability bands on the plot
  lines(x = interval,
        y = prob_bands$upper,
        col = "blue",
        lwd = 2)
  lines(x = interval,
        y = prob_bands$lower,
        col = "blue",
        lwd = 2)
  
  # Add our observations
  points(x = observations$x,
         y = observations$y,
         col = "black",
         pch = 16)
  
  # Add legend
  legend('topright',
         legend = c("Posterior mean of f", "95% PB for f", "Observations"),
         col = c("black", "blue", "black"),
         lty = c(1, 1, NA),
         lwd = c(1,2, NA),
         pch = c(NA, NA, 16),
         cex = 0.8)
}
```

With hyper parameters $\sigma_f = 1$ and $l=0.3$ we get the following posterior mean for *f* over the interval $x \in [-1,1]$, the black line. Lines in blue illustrates 95% probability bands for *f* and the black dot illustrates our observation $(x,y) = (0.4, 0.719)$. 

```{r, echo = FALSE}
plot_post_mean(f_posterior$Mean, f_posterior$Variance,x_interval,obs)
```

## Description 1.3 - Two Observations Mean and Probability Bands for f
Update your posterior from (2) with another observation: $(x,y) = (-0.6,-0.044)$. Plot the posterior mean of *f* over the interval $x \in [-1,1]$. Plot also 95 % probability (point-wise) bands for *f*.
**Hint**: Updating the posterior after one observation with a new observation gives the
same result as updating the prior directly with the two observations.

## Solution 1.3 - Two Observations Mean and Probability Bands for f
Updating our posterior with another observation $(x,y) = (-0.6, -0.044)$ we get the following plot on the same interval. When comparing the two graphs we note that the posterior mean of *f* has changed. 
```{r, echo = FALSE}
# Update with 2 observations
obs <- data.frame(x = c(0.4,-0.6), y = c(0.719,-0.044))

# Get the posterior mean and variance of f
f_posterior <- posteriorGP(x = obs$x,
                           y = obs$y,
                           xStar = x_interval,
                           sigmaNoise = sigmaNoise,
                           k = SEkernel,
                           sigmaf = sigmaf,
                           l = ell)

# Plot the posterior mean of f, observations, and 95% probability bands for f
plot_post_mean(f_posterior$Mean, f_posterior$Variance, x_interval, obs)
```

## Description 1.4 - Five Observations Mean and Probability Bands for f
Compute the posterior distribution of *f* using all the five data points in the table below (note that the two previous observations are included in the table). Plot the posterior mean of *f* over the interval $x \in [-1,1]$. Plot also 95 % probability (pointwise) bands for *f*.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
x & -1.0 & -0.6 & -0.2 & 0.4 & 0.8 \\
\hline
y & 0.768 & -0.044 & -0.940 & 0.719 & -0.664 \\
\hline
\end{tabular}
\end{center}

## Solution 1.4 - Five Observations Mean and Probability Bands for f
Updating our posterior with five observations (three new) we get the following plot on the same interval. When comparing the this with our previous graphs we once again note that the posterior mean of f has changed.
```{r, echo =FALSE}
# Update with 5 observations
obs <- data.frame(x = c(-1.0,-0.6,-0.2,0.4,0.8), 
                  y = c(0.768,-0.044,-0.940,0.719,-0.664))

# Get the posterior mean and variance of f
f_posterior <- posteriorGP(x = obs$x,
                           y = obs$y,
                           xStar = x_interval,
                           sigmaNoise = sigmaNoise,
                           k = SEkernel,
                           sigmaf = sigmaf,
                           l = ell)

# Plot the posterior mean of f, observations, and 95% probability bands for f
plot_post_mean(f_posterior$Mean, f_posterior$Variance, x_interval, obs)
```

## Description 1.5 - Five Observations New Hyperparameters
Repeat (4), this time with hyperparameters $\sigma_f = 1$ and $l=1$. Compare the results.

## Solution 1.5 - Five Observations New Hyperparameters
Using the same observations as above but with hyperparamters $\sigma_f = 1$ and $l=1$ we get the plot below. First and foremost we note that we only update the *l* hyperparameter compared with our previous plot. *l* controls the smoothness of the function and thus as *l* gets larger the smoothness of the function increases, which can be observed when comparing this plot with our previous one. 
```{r, echo = FALSE}
# Update parameters
sigmaf <- 1
ell <- 1

# Get the posterior mean and variance of f
f_posterior <- posteriorGP(x = obs$x,
                           y = obs$y,
                           xStar = x_interval,
                           sigmaNoise = sigmaNoise,
                           k = SEkernel,
                           sigmaf = sigmaf,
                           l = ell)

# Plot the posterior mean of f, observations, and 95% probability bands for f
plot_post_mean(f_posterior$Mean, f_posterior$Variance, x_interval, obs)
```

# Task 2 GP Regression with kernlab
## Description 2.1 - Define Squared Exponential Kernel
In this exercise, you will work with the daily mean temperature in Stockholm (Tullinge) during the period January 1, 2010 - December 31, 2015. We have removed the leap year day February 29, 2012 to make things simpler. You can read the dataset with the command: read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/
Code/TempTullinge.csv", header=TRUE, sep=";")

Create the variable *time* which records the day number since the start of the dataset (i.e., *time*= 1, 2, . . ., 365 × 6 = 2190). Also, create the variable *day* that records the day number since the start of each year (i.e., *day*= 1, 2, . . ., 365, 1, 2, . . ., 365). Estimating a GP on 2190 observations can take some time on slower computers, so let us subsample the data and use only every fifth observation. This means that your time and day variables are now *time*= 1, 6, 11, ..., 2186 and *day*= 1, 6, 11, ..., 361, 1, 6, 11, ..., 361.

Familiarize yourself with the functions *gausspr* and in *kernlab.* Do *?gausspr* and read the input arguments and the output. Also, go through the file *KernLabDemo.R* available on the course website. You will need to understand it. Now, define your own square exponential kernel function (with parameters *l (ell) * and $\sigma_f$ (sigmaf)), evaluate it in the point $x = 1$, $x' = 2$, and use the *kernelMatrix* function to compute the covariance matrix $K(X,X_*)$ for the input vectors $X=(1,3,4)^T$ and $X_* = (2,3,4)^T$.

## Solution 2.1 - Define Squared Exponential Kernel
We define our squared exponential kernel according to the code-block below. When evaluating our kernel function in the point $x = 1$, $x' = 2$ we get the output of $(0.6065307,  0.6065307)$. Further we get the matrix below using the *kernelMatrix* function to compute the covariance matrix $K(X,X_*)$ for the input vectors $X=(1,3,4)^T$ and $X_* = (2,3,4)^T$. We obtain 1 in [2,2] and [3,3] because we have the same values in our input vector for the second and third element. 

```{r, echo = FALSE}
#Import the dataset
df <-  read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv",
                header=TRUE, sep=";")

# Create the variable time which records the day number since the start of the dataset
time <- seq(1,2190,1)

# Create the variable day that records the day number since the start of each year 
day <- c(rep(seq(1,365,1),6))

# Get every fifth time and day
observations <- data.frame(temp = df$temp[time[seq(1, length(time), by = 5)]], 
                           time = time[seq(1, length(time), by = 5)],
                           day = day[seq(1, length(day), by = 5)])


# Squared exponential kernel function
SEkernel_nestled <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x1, x2 = NULL) {
    return(sigmaf^2*exp(-(x1-x2)^2/(2*ell^2)))
  }
  class(rval) <- "kernel"
  return(rval)
} 

SEkernelFunc <- SEkernel_nestled(sigmaf = 1, ell = 1) # Our kernel FUNCTION.
SEkernelFunc(c(1,1),c(2,2)) # Evaluating the kernel in x=c(1,1), x'=c(2,2).

# Testing our own defined kernel function.
X <- matrix(c(1,3,4)) # Simulating some data.
Xstar <- matrix(c(2,3,4))

# Computing the whole covariance matrix K from the kernel.
kernelMatrix(kernel = SEkernelFunc, x = X, y = Xstar) # So this is K(X,Xstar).
```

## Description 2.2 - GP Regression, Temp as a Function of Time
Consider first the following model: 
$$temp = f(time) + \epsilon \text{ with } \epsilon \sim N(0, \sigma^{2}_{n}) \text{ and } f \sim GP(0, k(time, time'))$$
Let $\sigma^{2}_{n}$ be the residual variance from a simple quadratic regression fit (using the *lm* function in R). Estimate the above Gaussian process regression model using the squared exponential function from (1) with $\sigma_f=20$ and $l = 0.2$. Use the predict function in R to compute the posterior mean at every data point in the training dataset. Make a scatterplot of the data and superimpose the posterior mean of *f* as a curve (use *type="l"* in the plot function). Play around with different values on $\sigma_f$  and $l$ (no need to write this in the report though).

## Solution 2.2 - GP Regression, Temp as a Function of Time
Considering the following model above: We get the plot below when scatterplotting the data (black dots) and the posterior mean of $f$ (red line) with $\sigma_f=20$ and $l = 0.2$. Notable is that the posterior mean of $f$ seams to fit the data pretty well.
```{r, echo = FALSE}
# Linear model to obtain sigmaNoise
lmfit <- lm(formula = temp ~ time + I(time^2), data = observations)
sigmaNoise <- sd(lmfit$residuals)

# Set the parameters for our kernel function
SEkernelFunc <- SEkernel_nestled(sigmaf = 20, ell = 0.2)

# Fit the GP with our SEkernel function
GPfit <- gausspr(y = observations$temp,
                 x = observations$time, 
                 kernel = SEkernelFunc, 
                 var = sigmaNoise ^ 2)

# Predicting the mean for the training data
pred.temp.mean <- predict(GPfit, observations$time) 
```

```{r, echo=FALSE}
# Plot our observations and the posterior mean of f
plot(observations$time, observations$temp, xlab = "Time", ylab = "Temperature", 
     pch = 1, main = "GP Regression, function of time, SEkernel")
lines(observations$time, pred.temp.mean, type = "l", col = "red")
legend('bottomright',
       legend = c("Posterior mean of f", "Observations"),
       col = c("red","black"),
       lty = c(1, NA),
       pch = c(NA, 1),
       cex = 0.8)
```

## Description 2.3 - GP Regression add Probability Bands
*kernlab* can compute the posterior variance of *f*, but it seems to be a bug in the code. So, do your own computations for the posterior variance of *f* and plot the 95 % probability (pointwise) bands for *f*. Superimpose these bands on the figure with the posterior mean that you obtained in (2).
**Hint**: Note that Algorithm 2.1 on page 19 of Rasmussen and Willams’ book already does the calculations required. Note also that *kernlab* scales the data by default to have zero mean and standard deviation one. So, the output of your implementation of Algorithm 2.1 will not coincide with the output of *kernlab* unless you scale the data first. For this, you may want to use the R function *scale.* When plotting the results, do not forget to unscale by multiplying with the standard deviation and adding the mean.

## Solution 2.3 - GP Regression add Probability Bands
Plotting the 95% probability bands (blue lines) for $f$ we get the plot below. 
```{r, echo=FALSE}
# Initiate parameters
sigmaf = 20
ell = 0.2

#Scale the x and y -> (val-mean)/sd
scaled.time <- scale(observations$time)
scaled.temp <- scale(observations$temp)

# Linear model to get the new noise for scaled data
lmfit.scaled <- lm(scaled.temp ~ scaled.time + I(scaled.time^2))
sigmaNoise.scaled <- var(lmfit.scaled$residuals)

# Get the posterior mean and variance of f
f_posterior <- posteriorGP(x = scaled.time,
                           y = scaled.temp,
                           xStar = scaled.time,
                           sigmaNoise = sigmaNoise.scaled,
                           k = SEkernel,
                           sigmaf = sigmaf,
                           l = ell)

# Here we use the mean obtained from the gausspr package and unscale our 
# standard deviation to get the upper and lower probability bands for f
upper_bound <-  pred.temp.mean + 1.96 * sqrt(f_posterior$Variance)*sd(observations$temp)
lower_bound <-  pred.temp.mean - 1.96 * sqrt(f_posterior$Variance)*sd(observations$temp)
```

```{r, echo=FALSE}
# Plot the posterior mean of f, observations, and 95% probability bands for f
plot(x = observations$time, 
     y = observations$temp, 
     xlab = "Time", 
     ylab = "Temperature", 
     pch = 1, 
     main = "GP regression, function of time, SEkernel",
     ylim = c(-22,22))
#ylim = c(min(lower_bound)-1,max(upper_bound)+1)
lines(observations$time, pred.temp.mean, type = "l", col = "red")
lines(observations$time, upper_bound, type = "l", col = "blue")
lines(observations$time, lower_bound, type = "l", col = "blue")
# Add legend
legend('bottomright', 
       legend = c("Posterior mean of f", "95% PB of f", "Observations"),
       col = c("red", "blue", "black"),
       lty = c(1, 1, NA),
       lwd = c(1, 2, NA),
       pch = c(NA, NA, 16),
       cex = 0.8)
```

## Description 2.4 - GP Regression, Temp as a Function of Day
Consider now the following model: 
$$temp = f(day) + \epsilon \text{ with } \epsilon \sim N(0, \sigma^{2}_{n}) \text{ and } f \sim GP(0, k(day, day'))$$
Estimate the model using the squared exponential function with $\sigma_f=20$ and $l = 0.2$. Superimpose the posterior mean from this model on the posterior mean from the model in (2). Note that this plot should also have the time variable on the horizontal axis.

## Solution 2.4 - GP Regression, Temp as a Function of Day
We get the plot below when scatterplotting the data (black dots) and the posterior mean of $f$ (blue line) with $\sigma_f=20$ and $l = 0.2$. The red line is the posterior mean of $f$ from the previous model. Notable is that the posterior mean of $f$ with temp as a model of day seams to fit the data pretty well but that we obtain the same posterior mean for every year. I.e., this model only takes what day it is into consideration and not data from previous years and trends over the years.

```{r, echo=FALSE}
# Linear model to obtain sigmaNoise
lmfit <- lm(formula = temp ~ day + I(day^2), data = observations)
sigmaNoise <- sd(lmfit$residuals)

# Set the parameters for our kernel function
SEkernelFunc <- SEkernel_nestled(sigmaf = 20, ell = 0.2)

# Fit the GP with our SEkernel function
GPfit <- gausspr(y = observations$temp,
                 x = observations$day, 
                 kernel = SEkernelFunc, 
                 var = sigmaNoise ^ 2)

# Predicting the training data
pred.tempday.mean <- predict(GPfit, observations$day) 

# Plot our observations and the posterior mean of f
plot(observations$time, observations$temp, xlab = "Time", ylab = "Temperature", 
     pch = 1, main = "GP using time and day")
lines(observations$time, pred.temp.mean, type = "l", col = "red")
lines(observations$time, pred.tempday.mean, type = "l", col = "blue")
# Add legend
legend('bottomright',
       title = "Posterior mean of f",
       legend = c("Time model", "Day model", "Observations"),
       col = c("red", "blue", "black"),
       lty = c(1, 1, NA),
       lwd = c(1, 2, NA),
       pch = c(NA, NA, 16),
       cex = 0.8)
```

## Description 2.5 - Periodic Kernel
Finally, implement the following extension of the squared exponential kernel with a periodic kernel (a.k.a. locally periodic kernel):
$$ k(x,x')=\sigma^{2}_{f} \text{ exp} \left\{ - \frac{2\sin^{2}(\pi |x-x'|/d)} {l^{2}_1} \right\} 
\text{exp} \left\{ - \frac{1}{2} \frac{|x-x'|^2}{l^{2}_{2}} \right\}$$
Note that we have two different length scales in the kernel. Intuitively, $l_1$ controls the correlation between two days in the same year, and $l_2$ controls the correlation between the same day in different years. Estimate the GP model using the time variable with this kernel and hyperparameters $\sigma_f = 20$, $l_1 = 1$, $l_2 = 10$ and $d = 365/sd(time)$. The reason for the rather strange period here is that *kernlab* standardizes the inputs to have standard deviation of 1. Compare the fit to the previous two models (with $\sigma_f=20$ and $l = 0.2$). Discuss the results.

## Solution 2.5 - Logically Periodic Kernel
Finally we implement an extension of the squared exponential kernel with a periodic kernel. Estimating the GP model using the time variable, this new kernel and the parameters $\sigma_f=20$, $l_1 = 1$, $l_2 = 10$, and $d = 365/sd(time)$ we can plot the posterior mean of $f$ (green line) in the plot above. When doing so we get the plot below.  Comparing the three lines we don’t note any significant differences appart from the blue line staying the same over the years (as said before). Between the green line and the red we don’t note any mayor differences, if anything the green line seem to follow the appearance of the blue with the given parameters. 

```{r, echo=FALSE}
# Logically periodic kernel function
LPkernel <- function(sigmaf = 1, ell1 = 1, ell2 = 1, d = 1) 
{
  rval <- function(x1, x2 = NULL) {
    return(sigmaf ^2 * exp(-2 * (sin(pi * abs(x1 - x2) / d)) ^ 2 / ell1 ^ 2)
                    * exp(-1/2 * abs(x1 - x2) ^ 2 / ell2 ^ 2))
  }
  class(rval) <- "kernel"
  return(rval)
} 

sigmaf <- 20
ell1 <- 1
ell2 <- 10
d <- 365/sd(observations$time)

# Linear model to obtain sigmaNoise
lmfit <- lm(formula = temp ~ time + I(time^2), data = observations)
sigmaNoise <- sd(lmfit$residuals)

# Set the parameters for our kernel function
kernelFunc <- LPkernel(sigmaf = sigmaf, ell1 = ell1, ell2 = ell2, d = d)

# Fit the GP with our SEkernel function
GPfit <- gausspr(y = observations$temp,
                 x = observations$time, 
                 kernel = kernelFunc, 
                 var = sigmaNoise ^ 2)

# Predicting the training data
pred.mean.SE3 <- predict(GPfit, observations$time) 

# Plot our observations and the posterior mean of f
plot(observations$time, observations$temp, xlab = "Time", ylab = "Temperature", 
     pch = 1, main = "GP Regression, Locally Periodic Kernel")
lines(observations$time, pred.mean.SE3, type = "l", col = "green")
lines(observations$time, pred.tempday.mean, type = "l", col = "blue")
lines(observations$time, pred.temp.mean, type = "l", col = "red")
# Add legend
legend('bottomright',
       title = "Posterior mean of f",
       legend = c("Time LPkernel", "Day SEkernel", "Time SEkernel", "Observations"),
       col = c("green", "blue", "red","black"),
       lty = c(1, 1, 1, NA),
       pch = c(NA, NA, NA, 1),
       cex = 0.8)
```

# Task 3 GP Classification with kernlab
## Description 3.1 - Gaussian Process Classification
Download the banknote fraud data:
```{r}
data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/banknoteFraud.csv",
                 header=FALSE, sep=",")
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud") 
data[,5] <- as.factor(data[,5])
```
You can read about this dataset here. Choose 1000 observations as training data using
the following command (i.e., use the vector SelectTraining to subset the training observa- tions):
```{r}
set.seed(111); 
SelectTraining <- sample(1:dim(data)[1], size = 1000, replace = FALSE)
```
Use the R package *kernlab* to fit a Gaussian process classification model for fraud on the training data. Use the default kernel and hyperparameters. Start using only the covariates *varWave* and *skewWave* in the model. Plot contours of the prediction probabilities over a suitable grid of values for *varWave* and *skewWave.* Overlay the training data for fraud = 1 (as blue points) and fraud = 0 (as red points). You can reuse code from the file *KernLabDemo.R* available on the course website. Compute the confusion matrix for the classifier and its accuracy.

## Solution 3.1 - Gaussian Process Fraud Classification
The circles with respective probability represent the probability of an observation being fraud and the points are the actual fraud values from the training data. Looking at the plot we note that fraud from the training data (in blue) are typically associated with a lower varWave but that there are some outliers. For some points we are as confident as 95% that it is fraud and for others 5% confident that there isn't (majority of the red points).
```{r, echo=FALSE}
# Divide the data
train <- data[SelectTraining,]
test <- data[-SelectTraining,]

# Fit the GP using 2 covariates varWave and skeWave
GPfitfraud <- gausspr(fraud ~  varWave + skewWave, data=train)

# Create intervals for x1 and x2 that captures all values in the training data
x1 <- seq(min(train$varWave),max(train$varWave),length=100)
x2 <- seq(min(train$skewWave),max(train$skewWave),length=100)

# Prepare data for plot
gridPoints <- meshgrid(x1, x2)
gridPoints <- cbind(c(gridPoints$x), c(gridPoints$y))

gridPoints <- data.frame(gridPoints)
names(gridPoints) <- names(train[1:2])
probPreds <- predict(GPfitfraud, gridPoints, type="probabilities")

# Plotting 
contour(x1,x2,matrix(probPreds[,2],100,byrow = TRUE), 20, xlab = "varWave", 
        ylab = "skewWave", main = "GP Fraud Classification")
points(train[train[,5]==1,1],train[train[,5]==1,2],col="blue")
points(train[train[,5]==0,1],train[train[,5]==0,2],col="red")

# Add legend
legend('bottomright',
       legend = c("1", "0"),
       col = c("blue", "red"),
       pch = c(1, 1),
       cex = 0.8)


# Accuracy function
accuracy <- function(X,X1){
  n <- length(X)
  return(sum(diag(table(X,X1)))/n)
}
```
Using the varWare and skewWave to predict fraud we get an accuracy of 0.941 for the training data and a confusion matrix as follows:
```{r, echo=FALSE}
# Confusion matrix for the training data
pred <- predict(GPfitfraud, newdata=train)
table(pred, train$fraud)
accuracy(pred, train$fraud) # Accuracy
```

## Description 3.2 - Predictions with the GP Classification Model
Using the estimated model from (1), make predictions for the test set. Compute the accuracy.

## Solution 3.2 - Predictions with the GP Classification Model
Using our model to make prediction for the test dataset we get an accuracy of 0.9247312 and the confusion matrix below:
```{r, echo=FALSE}
# Confusion matrix for the test data
pred <- predict(GPfitfraud, newdata=test)
table(pred, test$fraud)
accuracy(pred, test$fraud) # Accuracy
```
Comparing the two we note that the accuracy for the training data is higher than for the test data, which makes sense since the data is trained on the training data. The difference isn't that large though, suggesting the model generalize well. 

## Description 3.3 - Add Features to the GP Classification Model
Train a model using all four covariates. Make predictions on the test set and compare the accuracy to the model with only two covariates.

## Solution 3.3 - Add More Feautres to the GP Classification Model
Training a model using all four covariates we get an accuracy of 0.997 for the training data and 0.9946237 for the test data which is higher than what we got using only two covariates. This suggest that this second model that uses all four covariates are better since we obtain a higher accuracy for the test data.

```{r, echo=FALSE}
GPfitfraud <- gausspr(fraud ~., data=train)

gridPoints <- data.frame(gridPoints)
names(gridPoints) <- names(train[1:2])
probPreds <- predict(GPfitfraud, type="probabilities")

# Confusion matrix for the training data
pred <- predict(GPfitfraud, newdata=train)
table(pred, train$fraud)
accuracy(pred, train$fraud) # Accuracy

# Confusion matrix for the test data
pred <- predict(GPfitfraud, newdata=test)
table(pred, test$fraud)
accuracy(pred, test$fraud) # Accuracy
```